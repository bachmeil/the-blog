---
title: My involvement with D going forward
layout: post
---

I've been a heavy user of the D programming language since I started using it in 2013. The appealing thing about D both then and now is its potential as a glue language. Back in the old days, we'd talk about scripting languages as glue languages, with R and Tcl being two of the more prominent examples. D's strong interoperability with C made it possible to use D with languages such as R and Python. You could create R packages containing D code and others could use them from R like any other packages installed from CRAN. You could have R, Python, D, and C code in the same program.

The cherry on top was that the language had such a nice design. All the low-level power of C but with a nice, intuitive syntax. The learning curve was low for someone with a background in C or Fortran.

As a result, I invested a lot of my leisure time in D. Just a couple of them were [embedrv2](https://github.com/bachmeil/embedrv2) and [BetterR](https://bachmeil.github.io/betterr/). The former allows you to write libraries of D functions and handles all the boilerplate so that you can call them from R without doing any additional work. The latter is a very comprehensive solution for data science in D. I did a lot more, but these were the projects where I invested the time needed to make them accessible to others.

I'm going to state publicly something I've known for quite a while. There is no interest in D as a data science solution. I'm not talking about the usual use cases for R, Python, or Stata, where you're calling a canned routine as part of an interactive data analysis. That was never a possibility for any compiled language. I'm talking about cases requiring speed. For instance, in Bayesian inference, a simulation can run for days. As far as I can tell, nobody has ever used any of the work I've shared for anything.

I should also disclose that I would no longer recommend using D for those things anyway. The programming landscape has changed dramatically since 2013. These days my recommendation for an economics grad student wanting a fast language would be to use Julia. For someone wanting a traditional compiled language, I'd recommend looking at Swift and Kotlin, though I haven't done enough with either to actually recommend their use. Neither was an option in 2013 (Swift didn't even exist). There are others like Scala Native that have come along and are worth a look too.

As D's advantages of 2013 are no longer advantages in 2024, its weaknesses have become more important. The limitations on operator overloading are tough when you're doing numerical work. What appears to be good language design for some domains turns out to be a flat-out terrible choice for data science. Now, it does have some operator overloading, so the situation isn't as bad as it could be.

Another serious limitation of D is the package management situation. Seriously, you're supposed to write a JSON configuration file to go along with your program. What makes the situation orders of magnitude worse is that it's not necessary and it would be easy to fix.

The situation on Windows is pretty bad. This is not unique to D, but ginormous downloads and crossing your fingers hoping it will work and the fragility of working with DLLs is not something I want to deal with. The compiler could, but does not, offer any help.

A final problem with D is that I have lost all confidence in the direction things are going. Just one of many examples is what they call editions. A library author complained about the need to support too many compilers. Well there's a standard solution to that in open source projects. You put a particular label like "LTS" or "stable" on a release and then library authors only need to worry about supporting those releases. The D community is never one to be satisfied with simple solutions to simple problems, so they decided instead to go with a deeply overengineered solution referred to as editions. Sixteen months have passed, library authors still have to deal with a large number of compiler releases, and the concept of editions hasn't been specified. All I can do is shake my head.

The one that's most severe for me is the planned move to safe by default. I wouldn't care about this if there was an easy way to shut it off. After all, a language that's been developed for more than 20 years with C interoperability as the biggest selling point isn't the best candidate for safe by default. It's beyond obvious that this is a situation where you do something like add a compilation flag for safe mode. Nope. That's not happening for unstated reasons. The plan is to impose safety on everyone and to make it a pain in the ass to use unsafe code. It's hard for me to comprehend the decision process that leads us to these places. That would mean that in order for someone to use D for data science (which requires interoperability with other languages) they'd need to deal with *that*. Yikes.

One example that might not seem like a big deal to others is the lack of moderation on the D forums. I can't tell a student to post there if I don't know what someone else is going to post in response. Even if the responses are okay, it's not a good look to point to what is often a cesspool as representative of the language - it simply is not professional.

I want to be clear about something. This should not be interpreted as me saying D is dead or anything like that. It's still a good language and has a significant user base. *My work* with D has had no impact, so I'm going to stop investing my time in sharing it. I *would* be recommending to others that they use D for data science if there was a market for it, but no such market exists. I *am* going to keep using it for my own stuff, because there would be little benefit from jumping to another language, and doing so would come at a high cost. There are problems with the direction of the language, but that's true for any language that's used.